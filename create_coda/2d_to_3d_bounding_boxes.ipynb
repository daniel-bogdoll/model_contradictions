{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.colors as col\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import cv2\n",
    "from open3d import *\n",
    "from nuscenes.nuscenes import NuScenes, NuScenesExplorer\n",
    "from nuscenes.utils.data_classes import LidarPointCloud\n",
    "from nuscenes.utils.geometry_utils import view_points\n",
    "from pyquaternion import Quaternion\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.cluster import estimate_bandwidth\n",
    "from mpl_toolkits import mplot3d\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "\n",
    "kitti_path = '/disk/ml/datasets/KITTI/object/data/'\n",
    "calib_kitti_path = f'{kitti_path}training/calib/'\n",
    "\n",
    "nuscenes_path = '/disk/ml/datasets/nuScenes'\n",
    "\n",
    "once_path = '/disk/ml/datasets/ONCE/data_root/data/'\n",
    "\n",
    "coda_path = '/disk/ml/own_datasets/CODA/'\n",
    "projection_path = f'{coda_path}projection/'\n",
    "lidar_anno_path = f'{coda_path}lidar_annotation/'\n",
    "projection_anno_path = f'{coda_path}projection_annotation/'\n",
    "image_path = f'{coda_path}image/'\n",
    "binary_path = f'{coda_path}lidar/'\n",
    "nuscenes_indices = f'{coda_path}nuscenes_indices.json'\n",
    "corner_case = f'{coda_path}corner_case.json'\n",
    "    \n",
    "with open(nuscenes_indices, 'r') as file:\n",
    "    nuscenes_indices = json.load(file)\n",
    "    \n",
    "with open(corner_case, 'r') as file:\n",
    "    corner_case = json.load(file)\n",
    "    \n",
    "categories = corner_case['categories']\n",
    "images = corner_case['images']\n",
    "annotations = corner_case['annotations']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-trainval...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "64386 instance,\n",
      "12 sensor,\n",
      "10200 calibrated_sensor,\n",
      "2631083 ego_pose,\n",
      "68 log,\n",
      "850 scene,\n",
      "34149 sample,\n",
      "2631083 sample_data,\n",
      "1166187 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 58.782 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 8.8 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "nusc_trainval = NuScenes(version='v1.0-trainval', dataroot=nuscenes_path, verbose=True)\n",
    "nusc = NuScenesExplorer(nusc=nusc_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single_annotations = True\n",
    "weight_z_lidar = 0.1\n",
    "eps = 0.15\n",
    "min_samples = 6\n",
    "num_adjacent_points = 5\n",
    "quantile = 0.3\n",
    "\n",
    "show_all = True\n",
    "\n",
    "image_id = 1\n",
    "\n",
    "dpi = 400\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_normalized_pointcloud(cam, img, name, annotation_bbox, kitti=False, once=False):\n",
    "    #normalize\n",
    "    cam = normalize(cam)\n",
    "    \n",
    "    # filter point out of canvas\n",
    "    cam = cut_pointcloud(cam, annotation_bbox, img, once)\n",
    "    \n",
    "    \n",
    "    cam_norm = cam.copy().T\n",
    "    \n",
    "    #un_normalize\n",
    "    cam = un_normalize(cam, kitti)\n",
    "    \n",
    "    return cam, cam_norm\n",
    "\n",
    "def normalize(cam):\n",
    "    cam[:2] /= cam[2,:]\n",
    "    return cam\n",
    "\n",
    "def cut_pointcloud(cam, annotation_bbox, img, once):\n",
    "    \n",
    "    u,v,z = cam\n",
    "    if show_all:\n",
    "        img = Image.open(img)\n",
    "        img_w, img_h = img.size\n",
    "        u_out = np.logical_or(u<0, u>img_w)\n",
    "        v_out = np.logical_or(v<0, v>img_h)\n",
    "    else:\n",
    "        u_out = np.logical_or(u<annotation_bbox[0], u>annotation_bbox[0]+annotation_bbox[2])\n",
    "        v_out = np.logical_or(v<annotation_bbox[1], v>annotation_bbox[1]+annotation_bbox[3])\n",
    "    outlier = np.logical_or(u_out, v_out)\n",
    "    \n",
    "    return np.delete(cam,np.where(outlier),axis=1)\n",
    "\n",
    "def un_normalize(cam, kitti):\n",
    "    cam = cam.T\n",
    "    for row in cam:\n",
    "        if not kitti:\n",
    "            row[0] *= row[2]\n",
    "            row[1] *= row[2]\n",
    "        else:\n",
    "            row[0,0] *= row[0,2]\n",
    "            row[0,1] *= row[0,2]\n",
    "    cam = cam.T\n",
    "    cam = np.insert(cam,3,1,axis=0)\n",
    "    return cam\n",
    "\n",
    "def save_points(points, annotation_id):\n",
    "    points.astype('float32').tofile(f'{lidar_anno_path}{annotation_id}.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(cam, img, name, annotations, is_single_annotation=False, is_kitti=False):\n",
    "    \n",
    "    u,v,z,c1,c2,c3,c4 = cam\n",
    "    \n",
    "    # do projection staff\n",
    "    img = Image.open(img)\n",
    "    img_w, img_h = img.size\n",
    "    \n",
    "    img_overlay = Image.new('RGB', (img_w, img_h), (255,255,255))\n",
    "    \n",
    "    draw = ImageDraw.Draw(img)\n",
    "    draw_overlay = ImageDraw.Draw(img_overlay)\n",
    "    \n",
    "    for annotation in annotations:\n",
    "        if is_single_annotation:\n",
    "            anno_id = annotations['id']\n",
    "            bbox = annotations['bbox']\n",
    "        else:\n",
    "            anno_id = annotation['id']\n",
    "            bbox = annotation['bbox']\n",
    "        bbox_x = bbox[0]\n",
    "        bbox_y = bbox[1]\n",
    "        bbox_w = bbox[2]\n",
    "        bbox_h = bbox[3]\n",
    "        \n",
    "        #shape = [(bbox_x, bbox_y), (bbox_x + bbox_w, bbox_y + bbox_h)]\n",
    "        #draw.rectangle(shape, outline = (0,255,0), width = 3)\n",
    "        #draw_overlay.rectangle(shape, fill = (0,255,0), outline = (255,255,255), width = 3)\n",
    "        result = np.copy(np.array(img))\n",
    "        #result[~np.all(np.array(img_overlay) == 255*np.ones(3), axis=-1)] = 0.9*np.array(img)[~np.all(np.array(img_overlay) == 255*np.ones(3), axis=-1)] + 0.1*np.array(img_overlay)[~np.all(np.array(img_overlay) == 255*np.ones(3), axis=-1)]\n",
    "        \n",
    "        result_img = Image.fromarray(result.astype('uint8'))\n",
    "        \n",
    "    if is_single_annotation:\n",
    "        plot_figure(result_img, img_w, img_h, u, v, z, c1, c2, c3, c4, name=f'{anno_id}', save_path=f'{projection_anno_path}{anno_id}', is_kitti=is_kitti)    \n",
    "    else:\n",
    "        plot_figure(result_img, img_w, img_h, u, v, z, c1, c2, c3, c4, name=f'{name}', save_path=f'{projection_path}{name}', is_kitti=is_kitti)\n",
    "    \n",
    "    #plot_figure(result_img, img_w, img_h, u, v, z, c2)\n",
    "\n",
    "def plot_figure(img, img_w, img_h, u, v, z, c1, c2, c3, c4, name=\"\", save_path=\"\", show_plot=False, is_kitti=False):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,18),dpi=dpi,tight_layout=False)\n",
    "    ax = fig.add_subplot(411)\n",
    "    plt.title(name)\n",
    "    ax.imshow(img)\n",
    "    np_color = get_normalized_colors(c2, is_kitti)\n",
    "    ax.scatter([u],[v],c=np_color[:,:3],alpha=1,s=0.5)\n",
    "    \n",
    "    ax = fig.add_subplot(412)\n",
    "    ax.imshow(img)\n",
    "    np_color = get_normalized_colors(c4, is_kitti)\n",
    "    ax.scatter([u],[v],c=np_color[:,:3],alpha=1,s=0.5)\n",
    "    \n",
    "    ax = fig.add_subplot(425, projection='3d')\n",
    "    np_color = get_normalized_colors(c1, is_kitti)\n",
    "    ax.scatter([u],[z],[-v],c=np_color[:,:3],alpha=1,s=0.8)\n",
    "    \n",
    "    ax = fig.add_subplot(426, projection='3d')\n",
    "    np_color = get_normalized_colors(c2, is_kitti)\n",
    "    ax.scatter([u],[z],[-v],c=np_color[:,:3],alpha=1,s=0.8)\n",
    "    \n",
    "    ax = fig.add_subplot(427, projection='3d')\n",
    "    np_color = get_normalized_colors(c3, is_kitti)\n",
    "    ax.scatter([u],[z],[-v],c=np_color[:,:3],alpha=1,s=0.8)\n",
    "    \n",
    "    ax = fig.add_subplot(428, projection='3d')\n",
    "    np_color = get_normalized_colors(c4, is_kitti)\n",
    "    ax.scatter([u],[z],[-v],c=np_color[:,:3],alpha=1,s=0.8)\n",
    "    \n",
    "    if save_path:\n",
    "        #plt.savefig(f'{save_path}.png',bbox_inches='tight')\n",
    "        plt.savefig(f'/disk/no_backup/ju878/model_contradictions/create_coda/graphics/once_all_points.png',bbox_inches='tight')\n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "def get_normalized_colors(color, is_kitti=False):\n",
    "    cmap = cm.get_cmap('jet')\n",
    "    norm = col.Normalize(vmin=np.min(color), vmax=np.max(color))\n",
    "    np_colors = [cmap(norm(c)) for c in color]\n",
    "    np_color = np.asarray(np_colors)\n",
    "    if is_kitti:\n",
    "        np_color = np_color[0,0]\n",
    "    return np_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabels(points, cam_norm, center_point):\n",
    "    \n",
    "    db_labels = dbscan(points)\n",
    "    \n",
    "    points, cam_norm = setLabels(points, cam_norm, center_point, db_labels)\n",
    "    \n",
    "    ms_labels = meanshift(points)\n",
    "    \n",
    "    points, cam_norm = setLabels(points, cam_norm, center_point, ms_labels)\n",
    "    \n",
    "    \n",
    "    return points, cam_norm\n",
    "\n",
    "def dbscan(points):\n",
    "    \n",
    "    points_weighted = np.copy(points)\n",
    "    points_weighted[:,2] = weight_z_lidar * points_weighted[:,2]\n",
    "    \n",
    "    dbscan = DBSCAN(eps = eps, min_samples = min_samples).fit(points_weighted) # fitting the model\n",
    "    labels = np.array(dbscan.labels_).reshape(-1,1)\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def meanshift(points):\n",
    "    \n",
    "    bandwidth = estimate_bandwidth(points, quantile=quantile)\n",
    "    if bandwidth == 0.0:\n",
    "        bandwidth = 1.0\n",
    "    ms = MeanShift(bandwidth=bandwidth)\n",
    "    ms.fit(points)\n",
    "    labels = np.array(ms.labels_).reshape(-1,1)\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def setLabels(points, cam_norm, center_point, labels):\n",
    "    \n",
    "    points = np.hstack((points, labels.astype('float32')))\n",
    "    cam_norm = np.hstack((cam_norm, labels.astype('float32')))\n",
    "    \n",
    "    index_closest_point = get_closest_point(center_point, cam_norm)\n",
    "    label_closest_point = labels.copy()[index_closest_point]\n",
    "    labels_closest_point = labels.copy()\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == label_closest_point:\n",
    "            labels_closest_point[i] = 1\n",
    "        else:\n",
    "            labels_closest_point[i] = -1\n",
    "            \n",
    "    points = np.hstack((points, labels_closest_point.astype('float32')))\n",
    "    cam_norm = np.hstack((cam_norm, labels_closest_point.astype('float32')))\n",
    "    \n",
    "    return points, cam_norm\n",
    "\n",
    "def get_closest_point(center_point, points):\n",
    "    \n",
    "    diff = np.subtract(points[:,:2], center_point)\n",
    "    distance = np.einsum('ij,ij->i', diff, diff)\n",
    "    sorted_distance = np.argsort(distance)\n",
    "    return sorted_distance[np.argmin(points[sorted_distance[:num_adjacent_points],2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_frustum_once(name, img, binary, calib, annotation):\n",
    "    \n",
    "    annotation_id = annotation['id']\n",
    "    annotation_bbox = annotation['bbox']\n",
    "    \n",
    "    cam_to_velo = np.array(calib['calib']['cam03']['cam_to_velo'])\n",
    "    cam_intrinsic = np.array(calib['calib']['cam03']['cam_intrinsic'])\n",
    "    distortion = np.array(calib['calib']['cam03']['distortion'])\n",
    "    cam_intrinsic_n, _ = cv2.getOptimalNewCameraMatrix(cam_intrinsic, distortion, (1920, 1020), alpha=0.0, newImgSize=(1920, 1020))\n",
    "    cam_intrinsic = np.hstack([cam_intrinsic_n, np.zeros((3, 1), dtype=np.float32)])\n",
    "    \n",
    "    \n",
    "    scan = np.fromfile(binary, dtype=np.float32).reshape((-1,4))\n",
    "    points = scan[:, 0:3]\n",
    "     \n",
    "    points = np.insert(points,3,1,axis=1)\n",
    "    cam = np.dot(points, np.linalg.inv(cam_to_velo).T)\n",
    "    mask = cam[:,2] > 0\n",
    "    cam = cam[mask]\n",
    "    cam = np.dot(cam, cam_intrinsic.T)\n",
    "    cam = cam.T\n",
    "    \n",
    "    \n",
    "    #cut and normalize pointcloud\n",
    "    cam, cam_norm = cut_normalized_pointcloud(cam, img, name, annotation_bbox, once=True)\n",
    "    \n",
    "    \n",
    "    center_point = [annotation_bbox[0] + annotation_bbox[2] / 2, annotation_bbox[1] + annotation_bbox[3] / 2]\n",
    "    \n",
    "    cam_intrinsic = np.insert(cam_intrinsic,3,values=[0,0,0,1],axis=0)\n",
    "    points = np.dot(np.linalg.inv(cam_intrinsic), cam)\n",
    "    points = np.dot(cam_to_velo, points)\n",
    "    points = np.round(points,3)\n",
    "    points = points.T\n",
    "    \n",
    "    points, cam_norm = getLabels(points[:, :3], cam_norm, center_point)\n",
    "    \n",
    "    #save as bin\n",
    "    save_points(points, annotation_id)\n",
    "    if plot_single_annotations:\n",
    "        plot_graph(cam_norm.T, img, name, annotation, is_single_annotation=True)\n",
    "    \n",
    "    return cam_norm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_frustum_kitti(name, img, binary, calib, annotation):\n",
    "    \n",
    "    annotation_id = annotation['id']\n",
    "    annotation_bbox = annotation['bbox']\n",
    "    \n",
    "    # P2 (3 x 4) for left eye\n",
    "    P2 = np.matrix([float(x) for x in calib[2].strip('\\n').split(' ')[1:]]).reshape(3,4)\n",
    "    R0_rect = np.matrix([float(x) for x in calib[4].strip('\\n').split(' ')[1:]]).reshape(3,3)\n",
    "    # Add a 1 in bottom-right, reshape to 4 x 4\n",
    "    R0_rect = np.insert(R0_rect,3,values=[0,0,0],axis=0)\n",
    "    R0_rect = np.insert(R0_rect,3,values=[0,0,0,1],axis=1)\n",
    "    Tr_velo_to_cam = np.matrix([float(x) for x in calib[5].strip('\\n').split(' ')[1:]]).reshape(3,4)\n",
    "    Tr_velo_to_cam = np.insert(Tr_velo_to_cam,3,values=[0,0,0,1],axis=0)\n",
    "    Tr_cam_to_velo = np.matrix([float(x) for x in calib[6].strip('\\n').split(' ')[1:]]).reshape(3,4)\n",
    "    Tr_cam_to_velo = np.insert(Tr_cam_to_velo,3,values=[0,0,0,1],axis=0)\n",
    "\n",
    "    # read raw data from binary\n",
    "    scan = np.fromfile(binary, dtype=np.float32).reshape((-1,4))\n",
    "    points = scan[:, 0:3]# lidar xyz (front, left, up)\n",
    "    points = np.insert(points,3,1,axis=1).T\n",
    "    points = np.delete(points,np.where(points[0,:]<0),axis=1)\n",
    "    cam = P2 @ R0_rect @ Tr_velo_to_cam @ points\n",
    "    cam = np.delete(cam,np.where(cam[2,:]<0)[1],axis=1)\n",
    "    \n",
    "    #cut and normalize pointcloud\n",
    "    cam, cam_norm = cut_normalized_pointcloud(cam, img, name, annotation_bbox, kitti=True)\n",
    "    \n",
    "    center_point = [annotation_bbox[0] + annotation_bbox[2] / 2, annotation_bbox[1] + annotation_bbox[3] / 2]\n",
    "    \n",
    "    #inverse matrices\n",
    "    inv_Tr_velo_to_cam = np.linalg.inv(Tr_velo_to_cam)\n",
    "    inv_R0_rect = np.linalg.inv(R0_rect)\n",
    "    P2 = np.insert(P2,3,values=[0,0,0,1],axis=0)\n",
    "    inv_P2 = np.linalg.inv(P2)\n",
    "    \n",
    "    points = inv_Tr_velo_to_cam @ inv_R0_rect @ inv_P2 @ cam\n",
    "    points[3] = 0.0\n",
    "    points = np.round(points,3)\n",
    "    points = points.T\n",
    "    points, cam_norm = getLabels(points[:, :3], cam_norm, center_point)\n",
    "    #save as bin\n",
    "    save_points(points, annotation_id)\n",
    "    if plot_single_annotations:\n",
    "        plot_graph(cam_norm.T, img, name, annotation, is_single_annotation=True, is_kitti=True)\n",
    "    \n",
    "    return cam_norm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_frustum_nuscenes(name, img, binary, annotation):\n",
    "    \n",
    "    annotation_id = annotation['id']\n",
    "    annotation_bbox = annotation['bbox']\n",
    "    \n",
    "    # Get token ('nuscenes_033402.jpg': '1a41ba0751d5497ebd32df7c86950671')\n",
    "    token_nuscenes = nuscenes_indices[f'nuscenes_{name}.jpg']\n",
    "    \n",
    "    my_sample = nusc_trainval.get('sample', token_nuscenes)\n",
    "    cam = nusc_trainval.get('sample_data', my_sample['data']['CAM_FRONT'])\n",
    "    pointsensor = nusc_trainval.get('sample_data', my_sample['data']['LIDAR_TOP'])\n",
    "    \n",
    "    cs_record_p = nusc_trainval.get('calibrated_sensor', pointsensor['calibrated_sensor_token'])\n",
    "    poserecordp = nusc_trainval.get('ego_pose', pointsensor['ego_pose_token'])\n",
    "    poserecordc = nusc_trainval.get('ego_pose', cam['ego_pose_token'])\n",
    "    cs_record_c = nusc_trainval.get('calibrated_sensor', cam['calibrated_sensor_token'])\n",
    "    \n",
    "    \n",
    "    pc = LidarPointCloud.from_file(binary)\n",
    "    \n",
    "    \n",
    "    pc.rotate(Quaternion(cs_record_p['rotation']).rotation_matrix)\n",
    "    pc.translate(np.array(cs_record_p['translation']))\n",
    "\n",
    "    pc.rotate(Quaternion(poserecordp['rotation']).rotation_matrix)\n",
    "    pc.translate(np.array(poserecordp['translation']))\n",
    "    \n",
    "    pc.translate(-np.array(poserecordc['translation']))\n",
    "    pc.rotate(Quaternion(poserecordc['rotation']).rotation_matrix.T)\n",
    "    \n",
    "    pc.translate(-np.array(cs_record_c['translation']))\n",
    "    pc.rotate(Quaternion(cs_record_c['rotation']).rotation_matrix.T)\n",
    "    \n",
    "    cam = view_points(pc.points[:3, :], np.array(cs_record_c['camera_intrinsic']), normalize=False)\n",
    "    \n",
    "    cam = np.delete(cam,np.where(cam[2,:]<0)[0],axis=1)\n",
    "    \n",
    "    #cut and normalize pointcloud\n",
    "    cam, cam_norm = cut_normalized_pointcloud(cam, img, name, annotation_bbox)\n",
    "    \n",
    "    center_point = [annotation_bbox[0] + annotation_bbox[2] / 2, annotation_bbox[1] + annotation_bbox[3] / 2]\n",
    "    \n",
    "    \n",
    "    view = np.array(cs_record_c['camera_intrinsic'])\n",
    "    viewpad = np.eye(4)\n",
    "    viewpad[:view.shape[0], :view.shape[1]] = view\n",
    "    \n",
    "    pc.points = np.dot(np.linalg.inv(viewpad), cam)\n",
    "    \n",
    "    pc.rotate(Quaternion(cs_record_c['rotation']).rotation_matrix)\n",
    "    pc.translate(np.array(cs_record_c['translation']))\n",
    "    \n",
    "    pc.rotate(Quaternion(poserecordc['rotation']).rotation_matrix)\n",
    "    pc.translate(np.array(poserecordc['translation']))\n",
    "    \n",
    "    pc.translate(-np.array(poserecordp['translation']))\n",
    "    pc.rotate(Quaternion(poserecordp['rotation']).rotation_matrix.T)\n",
    "    \n",
    "    pc.translate(-np.array(cs_record_p['translation']))\n",
    "    pc.rotate(Quaternion(cs_record_p['rotation']).rotation_matrix.T)\n",
    "    \n",
    "    points = pc.points\n",
    "    points = np.round(points,3)\n",
    "    points = points.T\n",
    "    \n",
    "    if points.size == 0:\n",
    "        points = np.array([[1.0,1.0,1.0,1.0]])\n",
    "        cam_norm = np.array([[1.0,1.0,1.0]])\n",
    "    points, cam_norm = getLabels(points[:, :3], cam_norm, center_point)\n",
    "    \n",
    "    #save as bin\n",
    "    save_points(points, annotation_id)\n",
    "    if plot_single_annotations:\n",
    "        plot_graph(cam_norm.T, img, name, annotation, is_single_annotation=True)\n",
    "    \n",
    "    return cam_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for image in images:\n",
    "    if image['id'] == image_id:\n",
    "        image_id = image['id'] - 1\n",
    "        print(image_id)\n",
    "        id = image['id']\n",
    "        file_name = image['file_name'].split('.')[0]\n",
    "        prefix, name = file_name.split('_')\n",
    "        img = f'{image_path}{file_name}.jpg'\n",
    "        binary = f'{binary_path}{file_name}.bin'\n",
    "        annotations_in_image = []\n",
    "        annotations_in_cam = [-1,-1,-1,-1,-1,-1,-1]\n",
    "        \n",
    "        \n",
    "        for annotation in annotations:\n",
    "            if(annotation['image_id']) == id:\n",
    "                annotations_in_image.append(annotation)\n",
    "                #break\n",
    "        \n",
    "        \n",
    "        if prefix == 'kitti':\n",
    "            img = img.split('.')[0] + '.png'\n",
    "            with open(f'{calib_kitti_path}{name}.txt','r') as f:\n",
    "                calib = f.readlines()\n",
    "            for annotation in annotations_in_image:\n",
    "                annotations_in_cam = np.vstack([annotations_in_cam, create_frustum_kitti(name, img, binary, calib, annotation)])\n",
    "            plot_graph(annotations_in_cam.T, img, file_name, annotations_in_image, is_kitti=True)\n",
    "        elif prefix == 'nuscenes':\n",
    "            for annotation in annotations_in_image:\n",
    "                annotations_in_cam = np.vstack([annotations_in_cam, create_frustum_nuscenes(name, img, binary, annotation)])\n",
    "            plot_graph(annotations_in_cam.T, img, file_name, annotations_in_image)\n",
    "        else:\n",
    "            calib = json.load(open(f'{once_path}{prefix}/{prefix}.json', 'r'))\n",
    "            for annotation in annotations_in_image:\n",
    "                annotations_in_cam = np.vstack([annotations_in_cam, create_frustum_once(name, img, binary, calib, annotation)]) \n",
    "            plot_graph(annotations_in_cam.T, img, file_name, annotations_in_image)  \n",
    "        \n",
    "    \n",
    "    #plot_graph(annotations_in_cam.T, img, file_name, annotations_in_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
