{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 3D point wise clusters (runtime: ~ 2h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.colors as col\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from open3d import *\n",
    "from nuscenes.nuscenes import NuScenes, NuScenesExplorer\n",
    "from nuscenes.utils.data_classes import LidarPointCloud\n",
    "from nuscenes.utils.geometry_utils import view_points\n",
    "from pyquaternion import Quaternion\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.cluster import estimate_bandwidth\n",
    "from mpl_toolkits import mplot3d\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set paths\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<h4>ToDo:</h4>\n",
    "<ol>\n",
    "    <li> Set \"datasets_root\" to the root of your <b>dataset directory</b>\n",
    "    <li> Set \"coda_root\" to the root of your new <b>CODA directory</b>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_root = f'/disk/ml/datasets/'\n",
    "coda_root = f'/disk/ml/own_datasets/CODA/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<h4>ToDo:</h4>\n",
    "<ol>\n",
    "    <li> Set how much the x-axis is compressed for DBSCAN (0.1)\n",
    "    <li> Set \"eps\" for DBSCAN (0.15)\n",
    "    <li> Set \"min_samples\" for DBSCAN (6)\n",
    "    <li> Set \"quantile\" for mean shift (0.3)\n",
    "    <li> Set how many points around the center are considered for cluster selection (5)\n",
    "    <li> Select if all points (not only those in 2d bounding box) should be shown (False)\n",
    "    <li> Select if every single annotation should be plotted (False) (if True: runtime: ~ 10h)\n",
    "    <li> Set dpi for the plots for the manual inspection. (200 should be enough, 400 for print quality) (400 dpi: runtime: ~ 4h)\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbscan\n",
    "weight_x_lidar = 0.1\n",
    "eps = 0.15\n",
    "min_samples = 6\n",
    "\n",
    "# mean shift\n",
    "quantile = 0.3\n",
    "\n",
    "# cluster selection\n",
    "num_adjacent_points = 5\n",
    "\n",
    "show_all = False\n",
    "plot_single_annotations = True\n",
    "dpi = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "kitti_path = os.path.join(datasets_root, 'KITTI/object/data')\n",
    "calib_kitti_path = os.path.join(kitti_path, 'training/calib')\n",
    "\n",
    "nuscenes_path = os.path.join(datasets_root, 'nuScenes')\n",
    "\n",
    "once_path = os.path.join(datasets_root, 'ONCE/data_root/data')\n",
    "\n",
    "lidar_anno_path = os.path.join(coda_root, 'lidar_clustering')\n",
    "projection_path = os.path.join(coda_root, 'projection_clustering')\n",
    "projection_anno_path = os.path.join(coda_root, 'projection_clustering_annotation')\n",
    "image_path = os.path.join(coda_root, 'image')\n",
    "binary_path = os.path.join(coda_root, 'lidar')\n",
    "json_nuscenes_indices = os.path.join(coda_root, 'nuscenes_indices.json')\n",
    "json_corner_case = os.path.join(coda_root, 'corner_case.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(lidar_anno_path):\n",
    "        os.makedirs(lidar_anno_path)\n",
    "if not os.path.exists(projection_path):\n",
    "        os.makedirs(projection_path)\n",
    "if not os.path.exists(projection_anno_path):\n",
    "        os.makedirs(projection_anno_path)\n",
    "\n",
    "with open(json_nuscenes_indices, 'r') as file:\n",
    "    nuscenes_indices = json.load(file)\n",
    "    \n",
    "with open(json_corner_case, 'r') as file:\n",
    "    corner_case = json.load(file)\n",
    "    \n",
    "images = corner_case['images']\n",
    "annotations = corner_case['annotations']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load nuScenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nusc_trainval = NuScenes(version='v1.0-trainval', dataroot=nuscenes_path, verbose=True)\n",
    "nusc = NuScenesExplorer(nusc=nusc_trainval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save point clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_points(points, annotation_id):\n",
    "    points.astype('float32').tofile(os.path.join(lidar_anno_path, str(annotation_id) + '.bin'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to cut the point cloud to frustum size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_normalized_pointcloud(cam, img, name, annotation_bbox, kitti=False, once=False):\n",
    "    #normalize\n",
    "    cam = normalize(cam)\n",
    "    \n",
    "    # filter point out of canvas\n",
    "    cam = cut_pointcloud(cam, annotation_bbox, img, once)\n",
    "    \n",
    "    \n",
    "    cam_norm = cam.copy().T\n",
    "    \n",
    "    #un_normalize\n",
    "    cam = un_normalize(cam, kitti)\n",
    "    \n",
    "    return cam, cam_norm\n",
    "\n",
    "def normalize(cam):\n",
    "    cam[:2] /= cam[2,:]\n",
    "    return cam\n",
    "\n",
    "def cut_pointcloud(cam, annotation_bbox, img, once):\n",
    "    \n",
    "    u,v,z = cam\n",
    "    if show_all:\n",
    "        img = Image.open(img)\n",
    "        img_w, img_h = img.size\n",
    "        u_out = np.logical_or(u<0, u>img_w)\n",
    "        v_out = np.logical_or(v<0, v>img_h)\n",
    "    else:\n",
    "        u_out = np.logical_or(u<annotation_bbox[0], u>annotation_bbox[0]+annotation_bbox[2])\n",
    "        v_out = np.logical_or(v<annotation_bbox[1], v>annotation_bbox[1]+annotation_bbox[3])\n",
    "    outlier = np.logical_or(u_out, v_out)\n",
    "    \n",
    "    return np.delete(cam,np.where(outlier),axis=1)\n",
    "\n",
    "def un_normalize(cam, kitti):\n",
    "    cam = cam.T\n",
    "    for row in cam:\n",
    "        if not kitti:\n",
    "            row[0] *= row[2]\n",
    "            row[1] *= row[2]\n",
    "        else:\n",
    "            row[0,0] *= row[0,2]\n",
    "            row[0,1] *= row[0,2]\n",
    "    cam = cam.T\n",
    "    cam = np.insert(cam,3,1,axis=0)\n",
    "    return cam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to plot graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(cam, img, name, annotations, is_single_annotation=False, is_kitti=False):\n",
    "    \n",
    "    u,v,z,c1,c2,c3,c4 = cam\n",
    "    \n",
    "    # do projection staff\n",
    "    img = Image.open(img)\n",
    "    img_w, img_h = img.size\n",
    "    \n",
    "    img_overlay = Image.new('RGB', (img_w, img_h), (255,255,255))\n",
    "    \n",
    "    draw = ImageDraw.Draw(img)\n",
    "    draw_overlay = ImageDraw.Draw(img_overlay)\n",
    "    \n",
    "    for annotation in annotations:\n",
    "        if is_single_annotation:\n",
    "            anno_id = annotations['id']\n",
    "            bbox = annotations['bbox']\n",
    "        else:\n",
    "            anno_id = annotation['id']\n",
    "            bbox = annotation['bbox']\n",
    "        bbox_x = bbox[0]\n",
    "        bbox_y = bbox[1]\n",
    "        bbox_w = bbox[2]\n",
    "        bbox_h = bbox[3]\n",
    "        \n",
    "        shape = [(bbox_x, bbox_y), (bbox_x + bbox_w, bbox_y + bbox_h)]\n",
    "        draw.rectangle(shape, outline = (0,255,0), width = 3)\n",
    "        draw_overlay.rectangle(shape, fill = (0,255,0), outline = (255,255,255), width = 3)\n",
    "        result = np.copy(np.array(img))\n",
    "        result[~np.all(np.array(img_overlay) == 255*np.ones(3), axis=-1)] = 0.9*np.array(img)[~np.all(np.array(img_overlay) == 255*np.ones(3), axis=-1)] + 0.1*np.array(img_overlay)[~np.all(np.array(img_overlay) == 255*np.ones(3), axis=-1)]\n",
    "        \n",
    "        result_img = Image.fromarray(result.astype('uint8'))\n",
    "        \n",
    "    if is_single_annotation:\n",
    "        plot_figure(result_img, img_w, img_h, u, v, z, c1, c2, c3, c4, name=f'{anno_id}', save_path=os.path.join(projection_anno_path, str(anno_id)), is_kitti=is_kitti)    \n",
    "    else:\n",
    "        plot_figure(result_img, img_w, img_h, u, v, z, c1, c2, c3, c4, name=f'{name}', save_path=os.path.join(projection_path, name), is_kitti=is_kitti)\n",
    "    \n",
    "\n",
    "def plot_figure(img, img_w, img_h, u, v, z, c1, c2, c3, c4, name=\"\", save_path=\"\", show_plot=False, is_kitti=False):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,18),dpi=dpi,tight_layout=False)\n",
    "    ax = fig.add_subplot(411)\n",
    "    plt.title(name)\n",
    "    ax.imshow(img)\n",
    "    np_color = get_normalized_colors(c2, is_kitti)\n",
    "    ax.scatter([u],[v],c=np_color[:,:3],alpha=1,s=0.5)\n",
    "    \n",
    "    ax = fig.add_subplot(412)\n",
    "    ax.imshow(img)\n",
    "    np_color = get_normalized_colors(c4, is_kitti)\n",
    "    ax.scatter([u],[v],c=np_color[:,:3],alpha=1,s=0.5)\n",
    "    \n",
    "    ax = fig.add_subplot(425, projection='3d')\n",
    "    np_color = get_normalized_colors(c1, is_kitti)\n",
    "    ax.scatter([u],[z],[-v],c=np_color[:,:3],alpha=1,s=0.8)\n",
    "    \n",
    "    ax = fig.add_subplot(426, projection='3d')\n",
    "    np_color = get_normalized_colors(c2, is_kitti)\n",
    "    ax.scatter([u],[z],[-v],c=np_color[:,:3],alpha=1,s=0.8)\n",
    "    \n",
    "    ax = fig.add_subplot(427, projection='3d')\n",
    "    np_color = get_normalized_colors(c3, is_kitti)\n",
    "    ax.scatter([u],[z],[-v],c=np_color[:,:3],alpha=1,s=0.8)\n",
    "    \n",
    "    ax = fig.add_subplot(428, projection='3d')\n",
    "    np_color = get_normalized_colors(c4, is_kitti)\n",
    "    ax.scatter([u],[z],[-v],c=np_color[:,:3],alpha=1,s=0.8)\n",
    "    \n",
    "    if save_path:\n",
    "        \n",
    "        plt.savefig(f'{save_path}.png',bbox_inches='tight')\n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "def get_normalized_colors(color, is_kitti=False):\n",
    "    cmap = cm.get_cmap('jet')\n",
    "    norm = col.Normalize(vmin=np.min(color), vmax=np.max(color))\n",
    "    np_colors = [cmap(norm(c)) for c in color]\n",
    "    np_color = np.asarray(np_colors)\n",
    "    if is_kitti:\n",
    "        np_color = np_color[0,0]\n",
    "    return np_color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to cluster point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabels(points, cam_norm, center_point):\n",
    "    \n",
    "    db_labels = dbscan(points)\n",
    "    \n",
    "    points, cam_norm = setLabels(points, cam_norm, center_point, db_labels)\n",
    "    \n",
    "    ms_labels = meanshift(points)\n",
    "    \n",
    "    points, cam_norm = setLabels(points, cam_norm, center_point, ms_labels)\n",
    "    \n",
    "    \n",
    "    return points, cam_norm\n",
    "\n",
    "def dbscan(points):\n",
    "    \n",
    "    points_weighted = np.copy(points)\n",
    "    points_weighted[:,2] = weight_x_lidar * points_weighted[:,2]\n",
    "    \n",
    "    dbscan = DBSCAN(eps = eps, min_samples = min_samples).fit(points_weighted) # fitting the model\n",
    "    labels = np.array(dbscan.labels_).reshape(-1,1)\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def meanshift(points):\n",
    "    \n",
    "    bandwidth = estimate_bandwidth(points, quantile=quantile)\n",
    "    if bandwidth == 0.0:\n",
    "        bandwidth = 1.0\n",
    "    ms = MeanShift(bandwidth=bandwidth)\n",
    "    ms.fit(points)\n",
    "    labels = np.array(ms.labels_).reshape(-1,1)\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def setLabels(points, cam_norm, center_point, labels):\n",
    "    \n",
    "    points = np.hstack((points, labels.astype('float32')))\n",
    "    cam_norm = np.hstack((cam_norm, labels.astype('float32')))\n",
    "    \n",
    "    index_closest_point = get_closest_point(center_point, cam_norm)\n",
    "    label_closest_point = labels.copy()[index_closest_point]\n",
    "    labels_closest_point = labels.copy()\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == label_closest_point:\n",
    "            labels_closest_point[i] = 1\n",
    "        else:\n",
    "            labels_closest_point[i] = -1\n",
    "            \n",
    "    points = np.hstack((points, labels_closest_point.astype('float32')))\n",
    "    cam_norm = np.hstack((cam_norm, labels_closest_point.astype('float32')))\n",
    "    \n",
    "    return points, cam_norm\n",
    "\n",
    "def get_closest_point(center_point, points):\n",
    "    \n",
    "    diff = np.subtract(points[:,:2], center_point)\n",
    "    distance = np.einsum('ij,ij->i', diff, diff)\n",
    "    sorted_distance = np.argsort(distance)\n",
    "    return sorted_distance[np.argmin(points[sorted_distance[:num_adjacent_points],2])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to translate the point clouds into 2D image space, cut to frustum size, and perform the clustering to get the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ONCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_frustum_once(name, img, binary, calib, annotation):\n",
    "    \n",
    "    annotation_id = annotation['id']\n",
    "    annotation_bbox = annotation['bbox']\n",
    "    \n",
    "    cam_to_velo = np.array(calib['calib']['cam03']['cam_to_velo'])\n",
    "    cam_intrinsic = np.array(calib['calib']['cam03']['cam_intrinsic'])\n",
    "    distortion = np.array(calib['calib']['cam03']['distortion'])\n",
    "    cam_intrinsic_n, _ = cv2.getOptimalNewCameraMatrix(cam_intrinsic, distortion, (1920, 1020), alpha=0.0, newImgSize=(1920, 1020))\n",
    "    cam_intrinsic = np.hstack([cam_intrinsic_n, np.zeros((3, 1), dtype=np.float32)])\n",
    "    \n",
    "    \n",
    "    scan = np.fromfile(binary, dtype=np.float32).reshape((-1,4))\n",
    "    points = scan[:, 0:3]\n",
    "     \n",
    "    points = np.insert(points,3,1,axis=1)\n",
    "    cam = np.dot(points, np.linalg.inv(cam_to_velo).T)\n",
    "    mask = cam[:,2] > 0\n",
    "    cam = cam[mask]\n",
    "    cam = np.dot(cam, cam_intrinsic.T)\n",
    "    cam = cam.T\n",
    "    \n",
    "    \n",
    "    #cut and normalize pointcloud\n",
    "    cam, cam_norm = cut_normalized_pointcloud(cam, img, name, annotation_bbox, once=True)\n",
    "    \n",
    "    \n",
    "    center_point = [annotation_bbox[0] + annotation_bbox[2] / 2, annotation_bbox[1] + annotation_bbox[3] / 2]\n",
    "    \n",
    "    cam_intrinsic = np.insert(cam_intrinsic,3,values=[0,0,0,1],axis=0)\n",
    "    points = np.dot(np.linalg.inv(cam_intrinsic), cam)\n",
    "    points = np.dot(cam_to_velo, points)\n",
    "    points = np.round(points,3)\n",
    "    points = points.T\n",
    "    \n",
    "    points, cam_norm = getLabels(points[:, :3], cam_norm, center_point)\n",
    "    \n",
    "    #save as bin\n",
    "    save_points(points, annotation_id)\n",
    "    if plot_single_annotations:\n",
    "        plot_graph(cam_norm.T, img, name, annotation, is_single_annotation=True)\n",
    "    \n",
    "    return cam_norm\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KITTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_frustum_kitti(name, img, binary, calib, annotation):\n",
    "    \n",
    "    annotation_id = annotation['id']\n",
    "    annotation_bbox = annotation['bbox']\n",
    "    \n",
    "    # P2 (3 x 4) for left eye\n",
    "    P2 = np.matrix([float(x) for x in calib[2].strip('\\n').split(' ')[1:]]).reshape(3,4)\n",
    "    R0_rect = np.matrix([float(x) for x in calib[4].strip('\\n').split(' ')[1:]]).reshape(3,3)\n",
    "    # Add a 1 in bottom-right, reshape to 4 x 4\n",
    "    R0_rect = np.insert(R0_rect,3,values=[0,0,0],axis=0)\n",
    "    R0_rect = np.insert(R0_rect,3,values=[0,0,0,1],axis=1)\n",
    "    Tr_velo_to_cam = np.matrix([float(x) for x in calib[5].strip('\\n').split(' ')[1:]]).reshape(3,4)\n",
    "    Tr_velo_to_cam = np.insert(Tr_velo_to_cam,3,values=[0,0,0,1],axis=0)\n",
    "    Tr_cam_to_velo = np.matrix([float(x) for x in calib[6].strip('\\n').split(' ')[1:]]).reshape(3,4)\n",
    "    Tr_cam_to_velo = np.insert(Tr_cam_to_velo,3,values=[0,0,0,1],axis=0)\n",
    "\n",
    "    # read raw data from binary\n",
    "    scan = np.fromfile(binary, dtype=np.float32).reshape((-1,4))\n",
    "    points = scan[:, 0:3]# lidar xyz (front, left, up)\n",
    "    points = np.insert(points,3,1,axis=1).T\n",
    "    points = np.delete(points,np.where(points[0,:]<0),axis=1)\n",
    "    cam = P2 @ R0_rect @ Tr_velo_to_cam @ points\n",
    "    cam = np.delete(cam,np.where(cam[2,:]<0)[1],axis=1)\n",
    "    \n",
    "    #cut and normalize pointcloud\n",
    "    cam, cam_norm = cut_normalized_pointcloud(cam, img, name, annotation_bbox, kitti=True)\n",
    "    \n",
    "    center_point = [annotation_bbox[0] + annotation_bbox[2] / 2, annotation_bbox[1] + annotation_bbox[3] / 2]\n",
    "    \n",
    "    #inverse matrices\n",
    "    inv_Tr_velo_to_cam = np.linalg.inv(Tr_velo_to_cam)\n",
    "    inv_R0_rect = np.linalg.inv(R0_rect)\n",
    "    P2 = np.insert(P2,3,values=[0,0,0,1],axis=0)\n",
    "    inv_P2 = np.linalg.inv(P2)\n",
    "    \n",
    "    points = inv_Tr_velo_to_cam @ inv_R0_rect @ inv_P2 @ cam\n",
    "    points[3] = 0.0\n",
    "    points = np.round(points,3)\n",
    "    points = points.T\n",
    "    points, cam_norm = getLabels(points[:, :3], cam_norm, center_point)\n",
    "    #save as bin\n",
    "    save_points(points, annotation_id)\n",
    "    if plot_single_annotations:\n",
    "        plot_graph(cam_norm.T, img, name, annotation, is_single_annotation=True, is_kitti=True)\n",
    "    \n",
    "    return cam_norm\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nuScenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_frustum_nuscenes(name, img, binary, annotation):\n",
    "    \n",
    "    annotation_id = annotation['id']\n",
    "    annotation_bbox = annotation['bbox']\n",
    "    \n",
    "    # Get token ('nuscenes_033402.jpg': '1a41ba0751d5497ebd32df7c86950671')\n",
    "    token_nuscenes = nuscenes_indices[f'nuscenes_{name}.jpg']\n",
    "    \n",
    "    my_sample = nusc_trainval.get('sample', token_nuscenes)\n",
    "    cam = nusc_trainval.get('sample_data', my_sample['data']['CAM_FRONT'])\n",
    "    pointsensor = nusc_trainval.get('sample_data', my_sample['data']['LIDAR_TOP'])\n",
    "    \n",
    "    cs_record_p = nusc_trainval.get('calibrated_sensor', pointsensor['calibrated_sensor_token'])\n",
    "    poserecordp = nusc_trainval.get('ego_pose', pointsensor['ego_pose_token'])\n",
    "    poserecordc = nusc_trainval.get('ego_pose', cam['ego_pose_token'])\n",
    "    cs_record_c = nusc_trainval.get('calibrated_sensor', cam['calibrated_sensor_token'])\n",
    "    \n",
    "    \n",
    "    pc = LidarPointCloud.from_file(binary)\n",
    "    \n",
    "    \n",
    "    pc.rotate(Quaternion(cs_record_p['rotation']).rotation_matrix)\n",
    "    pc.translate(np.array(cs_record_p['translation']))\n",
    "\n",
    "    pc.rotate(Quaternion(poserecordp['rotation']).rotation_matrix)\n",
    "    pc.translate(np.array(poserecordp['translation']))\n",
    "    \n",
    "    pc.translate(-np.array(poserecordc['translation']))\n",
    "    pc.rotate(Quaternion(poserecordc['rotation']).rotation_matrix.T)\n",
    "    \n",
    "    pc.translate(-np.array(cs_record_c['translation']))\n",
    "    pc.rotate(Quaternion(cs_record_c['rotation']).rotation_matrix.T)\n",
    "    \n",
    "    cam = view_points(pc.points[:3, :], np.array(cs_record_c['camera_intrinsic']), normalize=False)\n",
    "    \n",
    "    cam = np.delete(cam,np.where(cam[2,:]<0)[0],axis=1)\n",
    "    \n",
    "    #cut and normalize pointcloud\n",
    "    cam, cam_norm = cut_normalized_pointcloud(cam, img, name, annotation_bbox)\n",
    "    \n",
    "    center_point = [annotation_bbox[0] + annotation_bbox[2] / 2, annotation_bbox[1] + annotation_bbox[3] / 2]\n",
    "    \n",
    "    \n",
    "    view = np.array(cs_record_c['camera_intrinsic'])\n",
    "    viewpad = np.eye(4)\n",
    "    viewpad[:view.shape[0], :view.shape[1]] = view\n",
    "    \n",
    "    pc.points = np.dot(np.linalg.inv(viewpad), cam)\n",
    "    \n",
    "    pc.rotate(Quaternion(cs_record_c['rotation']).rotation_matrix)\n",
    "    pc.translate(np.array(cs_record_c['translation']))\n",
    "    \n",
    "    pc.rotate(Quaternion(poserecordc['rotation']).rotation_matrix)\n",
    "    pc.translate(np.array(poserecordc['translation']))\n",
    "    \n",
    "    pc.translate(-np.array(poserecordp['translation']))\n",
    "    pc.rotate(Quaternion(poserecordp['rotation']).rotation_matrix.T)\n",
    "    \n",
    "    pc.translate(-np.array(cs_record_p['translation']))\n",
    "    pc.rotate(Quaternion(cs_record_p['rotation']).rotation_matrix.T)\n",
    "    \n",
    "    points = pc.points\n",
    "    points = np.round(points,3)\n",
    "    points = points.T\n",
    "    \n",
    "    if points.size == 0:\n",
    "        points = np.array([[1.0,1.0,1.0,1.0]])\n",
    "        cam_norm = np.array([[1.0,1.0,1.0]])\n",
    "    points, cam_norm = getLabels(points[:, :3], cam_norm, center_point)\n",
    "    \n",
    "    #save as bin\n",
    "    save_points(points, annotation_id)\n",
    "    if plot_single_annotations:\n",
    "        plot_graph(cam_norm.T, img, name, annotation, is_single_annotation=True)\n",
    "    \n",
    "    return cam_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create labeles for each annotation in every image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in tqdm(images):\n",
    "    id = image['id']\n",
    "    file_name = image['file_name'].split('.')[0]\n",
    "    prefix, name = file_name.split('_')\n",
    "    img = os.path.join(image_path, file_name + '.jpg')\n",
    "    binary = os.path.join(binary_path, file_name + '.bin')\n",
    "    annotations_in_image = []\n",
    "    annotations_in_cam = [-1,-1,-1,-1,-1,-1,-1]\n",
    "    \n",
    "    \n",
    "    for annotation in annotations:\n",
    "        if(annotation['image_id']) == id:\n",
    "            annotations_in_image.append(annotation)\n",
    "    \n",
    "    if prefix == 'kitti':\n",
    "        img = img.split('.')[0] + '.png'\n",
    "        with open(os.path.join(calib_kitti_path, name + '.txt'),'r') as f:\n",
    "            calib = f.readlines()\n",
    "        for annotation in annotations_in_image:\n",
    "            annotations_in_cam = np.vstack([annotations_in_cam, create_frustum_kitti(name, img, binary, calib, annotation)])\n",
    "        plot_graph(annotations_in_cam.T, img, file_name, annotations_in_image, is_kitti=True)\n",
    "    elif prefix == 'nuscenes':\n",
    "        for annotation in annotations_in_image:\n",
    "            annotations_in_cam = np.vstack([annotations_in_cam, create_frustum_nuscenes(name, img, binary, annotation)])\n",
    "        plot_graph(annotations_in_cam.T, img, file_name, annotations_in_image)\n",
    "    else:\n",
    "        calib = json.load(open(os.path.join(once_path, prefix, prefix + '.json'), 'r'))\n",
    "        for annotation in annotations_in_image:\n",
    "            annotations_in_cam = np.vstack([annotations_in_cam, create_frustum_once(name, img, binary, calib, annotation)]) \n",
    "        plot_graph(annotations_in_cam.T, img, file_name, annotations_in_image)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
